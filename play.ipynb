{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from model import RetNetModel, RetNetModelWithLMHead, RetNetConfig\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetNetModel(\n",
       "  (embedding): Embedding(100, 512)\n",
       "  (blocks): ModuleList(\n",
       "    (0-7): 8 x RetNetBlock(\n",
       "      (msr): MultiScaleRetention(\n",
       "        (qkv): Linear(in_features=512, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "        (gated): Linear(in_features=512, out_features=1024, bias=False)\n",
       "        (proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (gn): GroupNorm(4, 1024, eps=1e-05, affine=False)\n",
       "        (xpos): XPOS()\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "config = RetNetConfig(num_layers=8, vocab_size=100, hidden_size=512, num_heads=4, use_default_gamma=False, chunk_size=4)\n",
    "model = RetNetModel(config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "layer: 1\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 2\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 3\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 4\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 5\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 6\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 7\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 8\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([[1,2,3,4, 1,2,3,4]])\n",
    "\n",
    "out, parallel_past_kv = model(input_ids, forward_impl='parallel', return_kv=True)\n",
    "\n",
    "past_kv = None\n",
    "rnn_outs = []\n",
    "for i in range(input_ids.shape[1]):\n",
    "    rnn_out, past_kv = model(input_ids[:, i:i+1], forward_impl='recurrent', past_kv=past_kv, return_kv=True, sequence_offset=i)\n",
    "    rnn_outs.append(rnn_out)\n",
    "rnn_outs = torch.cat(rnn_outs, dim=1)\n",
    "\n",
    "chunk_out, chunk_past_kv = model(input_ids, forward_impl='chunkwise', return_kv=True)\n",
    "\n",
    "print(torch.allclose(out, rnn_outs, atol=1e-5))\n",
    "print(torch.allclose(out, chunk_out, atol=1e-5))\n",
    "print(torch.allclose(rnn_outs, chunk_out, atol=1e-5))\n",
    "\n",
    "for i, (p, r, c) in enumerate(zip(parallel_past_kv, past_kv, chunk_past_kv)):\n",
    "    print(f\"layer: {i + 1}\")\n",
    "    print(torch.allclose(p, r, atol=1e-5))\n",
    "    print(torch.allclose(p, c, atol=1e-5))\n",
    "    print(torch.allclose(r, c, atol=1e-5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[27, 17, 36, 96, 79, 51, 87, 97, 70, 16, 11,  2, 93, 68,  4, 62,  4, 62,\n",
       "           4, 62]]),\n",
       " tensor([[27, 17, 36, 96, 79, 51, 87, 97, 70, 16, 11,  2, 93, 68,  4, 62,  4, 62,\n",
       "           4, 62]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "config = RetNetConfig(num_layers=8, vocab_size=100, hidden_size=512, num_heads=4, use_default_gamma=False, chunk_size=4)\n",
    "model = RetNetModelWithLMHead(config)\n",
    "model.eval()\n",
    "\n",
    "p_generated = model.generate(input_ids, parallel_compute_prompt=True, max_new_tokens=20, do_sample=False, early_stopping=False)\n",
    "r_generated = model.generate(input_ids, parallel_compute_prompt=False, max_new_tokens=20, do_sample=False, early_stopping=False)\n",
    "\n",
    "p_generated, r_generated\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
